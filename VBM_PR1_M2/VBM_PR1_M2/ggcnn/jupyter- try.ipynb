{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "949ea90d-5a93-4bd7-85e5-992cb9ff5044",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import utils\n",
    "from models.ggcnn import GGCNN\n",
    "import numpy as np\n",
    "import scipy.ndimage as ndimage\n",
    "import time\n",
    "import math\n",
    "\n",
    "import cv2\n",
    "import tifffile as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc3b5287-3754-4fdc-a282-c471d58d137b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(depth, crop_size, out_size=300, return_mask=False, crop_y_offset=0):\n",
    "    # Crop\n",
    "    depth_crop = depth[(imh - crop_size) // 2 - crop_y_offset:(imh - crop_size) // 2 + crop_size - crop_y_offset,\n",
    "                               (imw - crop_size) // 2:(imw - crop_size) // 2 + crop_size]\n",
    "    \n",
    "    #Inpainting\n",
    "    depth_crop = cv2.copyMakeBorder(depth_crop, 1, 1, 1, 1, cv2.BORDER_DEFAULT)\n",
    "    depth_nan_mask = np.isnan(depth_crop).astype(np.uint8)\n",
    "\n",
    "    kernel = np.ones((3, 3),np.uint8)\n",
    "    depth_nan_mask = cv2.dilate(depth_nan_mask, kernel, iterations=1)\n",
    "\n",
    "    depth_crop[depth_nan_mask==1] = 0\n",
    "\n",
    "    # Scale to keep as float, but has to be in bounds -1:1 to keep opencv happy.\n",
    "    depth_scale = np.abs(depth_crop).max()\n",
    "    depth_crop = depth_crop.astype(np.float32) / depth_scale \n",
    "\n",
    "    depth_crop = cv2.inpaint(depth_crop, depth_nan_mask, 1, cv2.INPAINT_NS)\n",
    "\n",
    "    # Back to original size and value range.\n",
    "    depth_crop = depth_crop[1:-1, 1:-1]\n",
    "    depth_crop = depth_crop * depth_scale\n",
    "\n",
    "    depth_crop = cv2.resize(depth_crop, (out_size, out_size), cv2.INTER_AREA)\n",
    "\n",
    "    if return_mask:\n",
    "        depth_nan_mask = depth_nan_mask[1:-1, 1:-1]\n",
    "        depth_nan_mask = cv2.resize(depth_nan_mask, (out_size, out_size), cv2.INTER_NEAREST)\n",
    "        return depth_crop, depth_nan_mask\n",
    "    else:\n",
    "        return depth_crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b8efaa6-fe08-4ed0-8717-965c0b7f0cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GGCNN(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(9, 9), stride=(3, 3), padding=(3, 3))\n",
      "  (conv2): Conv2d(32, 16, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "  (conv3): Conv2d(16, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (convt1): ConvTranspose2d(8, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "  (convt2): ConvTranspose2d(8, 16, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n",
      "  (convt3): ConvTranspose2d(16, 32, kernel_size=(9, 9), stride=(3, 3), padding=(3, 3), output_padding=(1, 1))\n",
      "  (pos_output): Conv2d(32, 1, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (cos_output): Conv2d(32, 1, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (sin_output): Conv2d(32, 1, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (width_output): Conv2d(32, 1, kernel_size=(2, 2), stride=(1, 1))\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nehal/.local/lib/python3.10/site-packages/torch/serialization.py:1189: SourceChangeWarning: source code of class 'models.ggcnn.GGCNN' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/nehal/.local/lib/python3.10/site-packages/torch/serialization.py:1189: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/nehal/.local/lib/python3.10/site-packages/torch/serialization.py:1189: SourceChangeWarning: source code of class 'torch.nn.modules.conv.ConvTranspose2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    device = (\n",
    "            \"cuda\"\n",
    "            if torch.cuda.is_available()\n",
    "            else \"mps\"\n",
    "            if torch.backends.mps.is_available()\n",
    "            else \"cpu\"\n",
    "        )\n",
    "    model = torch.load('ggcnn_weights_cornell/ggcnn_epoch_23_cornell', map_location=torch.device(device), weights_only=False)\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c031082b-eb9e-4c2d-9e7d-be4a1a587912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "480   640\n"
     ]
    }
   ],
   "source": [
    "depth = cv2.imread('pcd0100d.tiff', -1)\n",
    "imh, imw = depth.shape\n",
    "print(imh, \" \", imw)\n",
    "out_size=300\n",
    "crop_y_offset=40\n",
    "crop_size=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d40f3e00-2ab3-42a9-b2c2-179b34df54b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(depth, crop_size, out_size, crop_y_offset=crop_y_offset):\n",
    "    depth, depth_nan_mask = pre_process(depth, crop_size, out_size, True, crop_y_offset=crop_y_offset)\n",
    "    # normalize\n",
    "    depth = np.clip((depth - depth.mean()), -1, 1)\n",
    "    tensor = torch.from_numpy(depth).float()\n",
    "    \n",
    "    tensor = torch.reshape(tensor, (1, 300, 300))\n",
    "    \n",
    "    pred_out = model(tensor)\n",
    "    \n",
    "    #pos, cos, sin, width\n",
    "    pred_out= np.array([pred_out[0].detach().numpy(),\n",
    "                        pred_out[1].detach().numpy(),\n",
    "                        pred_out[2].detach().numpy(),\n",
    "                        pred_out[3].detach().numpy()])\n",
    "    return pred_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bc7ba7a-5cc4-4209-80b3-1a06957f5ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_out=predict(depth, crop_size, out_size, crop_y_offset=crop_y_offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09517cb3-1597-437b-94fa-7d6aa240be63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1, 300, 300)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(pred_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4677578e-87bd-425a-92e4-180ee419d991",
   "metadata": {},
   "outputs": [],
   "source": [
    "def showImage(array, name_im):\n",
    "    array= np.reshape(array, (300,300))\n",
    "    grayImage = cv2.cvtColor(array, cv2.COLOR_GRAY2BGR)\n",
    "    cv2.imshow(name_im, grayImage)\n",
    "    cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18f918a1-4e46-4d25-9c74-a519c77f5e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 300, 300)\n",
      "[[[-0.00074773  0.00042186  0.00073207 ... -0.02339884 -0.019777\n",
      "   -0.00198394]\n",
      "  [ 0.00078627 -0.00645291 -0.00140484 ... -0.02787955 -0.02352439\n",
      "   -0.00152461]\n",
      "  [-0.00478718 -0.00146893 -0.01199936 ... -0.02854259 -0.02819814\n",
      "   -0.00735021]\n",
      "  ...\n",
      "  [-0.00935977 -0.01171742 -0.01715592 ... -0.01650656 -0.01007534\n",
      "   -0.01441366]\n",
      "  [-0.00963436 -0.01398444 -0.00634139 ... -0.00062511 -0.01076333\n",
      "   -0.01479367]\n",
      "  [-0.01287851 -0.01571235 -0.0165068  ... -0.01099557  0.00499863\n",
      "    0.01250987]]]\n"
     ]
    }
   ],
   "source": [
    "# showImage(\"pos\",pred_out[1])\n",
    "print(np.shape(pred_out[1]))\n",
    "print(pred_out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a924bd77-7827-4b49-a0db-033dd3243fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "showImage(pred_out[3], \"pos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f085559-3764-4e81-ab30-988b64a9c1bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117 212\n"
     ]
    }
   ],
   "source": [
    "argmax = np.argmax(pred_out[0])\n",
    "x= argmax%300\n",
    "y= int(np.ceil(argmax/300))\n",
    "\n",
    "print(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56658ede-cbf8-4e7e-98f1-5e4ecabe0e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.18177795410156\n",
      "0.044570039957761765\n"
     ]
    }
   ],
   "source": [
    "argmax = np.argmax(pred_out[0])\n",
    "width= pred_out[3][0][y][x]*150\n",
    "print(width)\n",
    "angle = 0.5 * np.arctan2(pred_out[2][0][y][x],pred_out[1][0][x][y]) \n",
    "print(angle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9c4d663-6153-4dcd-999d-5623a5e65b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "308 262 265 261\n"
     ]
    }
   ],
   "source": [
    "x=x+imw*0.5-150\n",
    "y=y+imh*0.5-150-crop_y_offset\n",
    "p1_x= int(x+(width*0.5*np.cos(angle)))\n",
    "p2_x= int(x-(width*0.5*np.cos(angle)))\n",
    "\n",
    "p1_y= int(y+(width*0.5*np.sin(angle)))\n",
    "p2_y= int(y-(width*0.5*np.sin(angle)))\n",
    "print(p1_x, p1_y, p2_x, p2_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1048cd54-79c7-489a-9e7a-9cac503cc5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "object= cv2.imread(\"pcd0100r.png\", cv2.IMREAD_COLOR)\n",
    "object= cv2.line(object,(p1_x,p1_y),(p2_x,p2_y),(255,0,0),5)\n",
    "cv2.imshow(\"Grasp\", object)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2293a3a-56cc-44c5-b50f-6f8c3b80cfc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
